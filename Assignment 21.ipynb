{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721b8805",
   "metadata": {},
   "source": [
    "# 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83943a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : The depth of a well-balanced binary tree containing m leaves is equal to log2(m), rounded up. A binary Decision Tree\n",
    "#       (one that makes only binary decisions, as is the case of all trees in Scikit-Learn) will end up more or less well\n",
    "#       balanced at the end of training, with one leaf per training instance if it is trained without restrictions. Thus, if \n",
    "#       the training set contains one million instances, the Decision Tree will have a depth of log2 (106) ≈ 20 (actually a bit\n",
    "#       more since the tree will generally not be perfectly well balanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295397f",
   "metadata": {},
   "source": [
    "# 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523621cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : A node's Gini impurity is generally lower than its parent's. This is ensured by the CART training algorithm's cost \n",
    "#       function, which splits each node in a way that minimizes the weighted sum of its children's Gini impurities. However,\n",
    "#       if one child is smaller than the other, it is possible for it to have a higher Gini impurity than its parent, as long\n",
    "#       as this increase is more than compensated for by a decrease of the other child's impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c80ad",
   "metadata": {},
   "source": [
    "# 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508e98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : If a Decision Tree is overfitting the training set, it may be a good idea to decrease max_depth, since this will \n",
    "#       constrain the model, regularizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97783646",
   "metadata": {},
   "source": [
    "# 4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae26d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : If a Decision Tree is underfitting the training set, it may be a good idea to increase max_depth, since this will \n",
    "#       constrain the model, regularizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46520fe9",
   "metadata": {},
   "source": [
    "# 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba7eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : The computational complexity of training a Decision Tree is O(n × m log(m)). So if you multiply the training set size\n",
    "#       by 10, the training time will be multiplied by K = (n × 10m × log(10m)) / (n × m × log(m)) = 10 × log(10m) / log(m).\n",
    "#       If m = 10^6, then K ≈ 11.7, so you can expect the training time to be roughly 11.7 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae2e942",
   "metadata": {},
   "source": [
    "# 6. Will setting presort=True speed up training if your training set has 100,000 instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc5db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : Presorting the training set speeds up raining only if the dataset is smaller than a few thousand instances. If it \n",
    "#       contains 100,000 instances, setting presort=True will considerably slow down training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8d973",
   "metadata": {},
   "source": [
    "# 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcb904",
   "metadata": {},
   "source": [
    "# a. To build a moons dataset, use make moons(n samples=10000, noise=0.4)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
