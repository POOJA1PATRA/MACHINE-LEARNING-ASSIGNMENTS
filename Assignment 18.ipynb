{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac08afb",
   "metadata": {},
   "source": [
    "# 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37cb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised learning \n",
    "#       uses labeled input and output data, while an unsupervised learning algorithm does not. Unsupervised learning models, \n",
    "#       in contrast, work on their own to discover the inherent structure of unlabeled data.\n",
    "#       n Supervised learning, you train the machine using data which is well “labeled.” Unsupervised learning is a machine\n",
    "#       learning technique, where you do not need to supervise the model. For example, Baby can identify other dogs based\n",
    "#       on past supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2466e56",
   "metadata": {},
   "source": [
    "# 2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e42548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : The main applications of unsupervised learning include clustering, visualization, dimensionality reduction, finding\n",
    "#       association rules, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c015f6",
   "metadata": {},
   "source": [
    "# 3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab9acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : The various types of clustering are:\n",
    "#       a) Connectivity-based Clustering (Hierarchical clustering) : Hierarchical Clustering is a method of unsupervised \n",
    "#          machine learning clustering where it begins with a pre-defined top to bottom hierarchy of clusters. It then proceeds\n",
    "#          to perform a decomposition of the data objects based on this hierarchy, hence obtaining the clusters. This method \n",
    "#          follows two approaches based on the direction of progress, i.e., whether it is the top-down or bottom-up flow of\n",
    "#          creating clusters.\n",
    "#       b) Centroids-based Clustering (Partitioning methods) : Centroid based clustering is considered as one of the most\n",
    "#          simplest clustering algorithms, yet the most effective way of creating clusters and assigning data points to it. The\n",
    "#          intuition behind centroid based clustering is that a cluster is characterized and represented by a central vector\n",
    "#          and data points that are in close proximity to these vectors are assigned to the respective clusters.  \n",
    "#       c) Density-based Clustering (Model-based methods) :If one looks into the previous two methods that we discussed, one\n",
    "#          would observe that both hierarchical and centroid based algorithms are dependent on a distance (similarity/proximity)\n",
    "#          metric. The very definition of a cluster is based on this metric. Density-based clustering methods take density\n",
    "#          into consideration \n",
    "#          instead of distances. Clusters are considered as the densest region in a data space, which is separated by regions \n",
    "#          of lower object density and it is defined as a maximal-set of connected points.\n",
    "#       d) Distribution-based Clustering : Until now, the clustering techniques as we know are based around either proximity \n",
    "#          (similarity/distance) or composition (density). There is a family of clustering algorithms that take a totally\n",
    "#          different metric into consideration – probability. Distribution-based clustering creates and groups data points \n",
    "#          based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial etc.) in the data.\n",
    "#       e) Fuzzy Clustering : The general idea about clustering revolves around assigning data points to mutually exclusive \n",
    "#          clusters, meaning, a data point always resides uniquely inside a cluster and it cannot belong to more than one\n",
    "#          cluster. Fuzzy clustering methods change this paradigm by assigning a data-point to multiple clusters with a \n",
    "#          quantified degree of belongingness metric. The data-points that are in proximity to the center of a cluster, may \n",
    "#          also belong in the cluster that is at a higher degree than points in the edge of a cluster. The possibility of which\n",
    "#          an element belongs to a given cluster is measured by membership coefficient that vary from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320db32f",
   "metadata": {},
   "source": [
    "# 4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71594c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS \n",
    "#       becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow. Within-Cluster-Sum of \n",
    "#       Squared Errors sounds a bit complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cc71d",
   "metadata": {},
   "source": [
    "# 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3356e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between \n",
    "#       points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means\n",
    "#       algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f22db",
   "metadata": {},
   "source": [
    "# 6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc2463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes.\n",
    "#       After each merging, the distances between all pairs of classes are updated. The distances at which the signatures of \n",
    "#       classes are merged are used to construct a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6c496",
   "metadata": {},
   "source": [
    "# 7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb2e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : I am going to refer to it as SSE, which stands for Sum of Squared Errors. The regression line is the line made using \n",
    "#       the function we defined above. To get the SSE we calculate the distance for each of the data points from the regression\n",
    "#       line then square the it, then we add to the sum.\n",
    "#       The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is \n",
    "#       a measure of error, the objective of k-means is to try to minimize this value. The purpose of this figure is to show \n",
    "#       that the initialization of the centroids is an important step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f463b061",
   "metadata": {},
   "source": [
    "# 8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a49df1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem.\n",
    "#       The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume \n",
    "#       k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "#       Step 1: Choose the number of clusters k. \n",
    "#       Step 2: Select k random points from the data as centroids.\n",
    "#       Step 3: Assign all the points to the closest cluster centroid. \n",
    "#       Step 4: Recompute the centroids of newly formed clusters.\n",
    "#       Step 5: Repeat steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd36958",
   "metadata": {},
   "source": [
    "# 9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1de7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest\n",
    "#       members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance). Complete-link\n",
    "#       clustering can also be described using the concept of clique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f116f65",
   "metadata": {},
   "source": [
    "# 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27033487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans : Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market \n",
    "#       Basket Analysis. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes,\n",
    "#       Mango} must also be frequent. Here is a dataset consisting of six transactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
